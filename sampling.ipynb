{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c8cf29",
   "metadata": {},
   "source": [
    "# Sampling linguistic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97072f0e",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by installing the [*conllu*](https://pypi.org/project/conllu/) library for Python, which allows parsing annotations in the CoNLL-U format.\n",
    "\n",
    "Note that the code below is not written in Python. This is a shell command, which is prefixed with an exclamation mark.\n",
    "\n",
    "Run the cell below to install the *conllu* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262be768",
   "metadata": {},
   "source": [
    "Let's import the necessary modules (`pd`), functions (`parse`) and classes (`Path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from conllu import parse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd0658",
   "metadata": {},
   "source": [
    "Next, we create a Path object that points towards the directory `gum_conllu` on the server, which contains annotations from the [Georgetown University Multilayer corpus](https://github.com/amir-zeldes/gum) (GUM).\n",
    "\n",
    "We then use the `glob` method to fetch every file (`*`) that has the suffix `conllu` stored in the directory.\n",
    "\n",
    "We store the resulting *generator* object under the variable `files`.\n",
    "\n",
    "We also create an empty list named `annotations` to hold the parsed annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path('gum_conllu').glob('*.conllu')\n",
    "\n",
    "annotations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffa25c",
   "metadata": {},
   "source": [
    "Next, we loop over the generator `files`, using the variable `file` to refer to each file yielded by the generator.\n",
    "\n",
    "We open each file for reading using the `open` function and store the result under the variable `annotation_file`.\n",
    "\n",
    "Next, we use the `read` method to read the contents of `annotation_file` and store the resulting string object under the variable `raw_annotation`.\n",
    "\n",
    "We then call the `parse` function imported from the *conllu* library to parse the string object stored under the variable `raw_annotation`. We store the result under the variable `parsed_annotation`.\n",
    "\n",
    "Finally, we append the parsed annotation to the list named `annotations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15919ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    with open(file, 'r') as annotation_file:\n",
    "        \n",
    "        raw_annotation = annotation_file.read()\n",
    "        \n",
    "        parsed_annotation = parse(raw_annotation)\n",
    "        \n",
    "        annotations.append(parsed_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8ad9d",
   "metadata": {},
   "source": [
    "Next, we create another empty list, which we assign under the variable `data`.\n",
    "\n",
    "We then loop over each annotated document in the list named `annotations`.\n",
    "\n",
    "For each text, we first retrieve metadata about the **genre** of the document and the unique identifier of the document.\n",
    "\n",
    "These are stored under the first sentence of the document, which may be accessed using the brackets and the number `[0]`.\n",
    "\n",
    "This information is stored under the attribute `metadata` under the keys `meta::genre` and `newdoc id`.\n",
    "\n",
    "Next, we loop over each sentence in the document.\n",
    "\n",
    "We retrieve the sentence type or *mood* of the sentence, stored in the metadata under the key `s_type`. In addition, we get document-level information for genre and the unique identifier. We also count the number of tokens in the sentence using Python's `len` function.\n",
    "\n",
    "We combine all of this information into a Python dictionary under the keys `mood`, `genre`, `document_id` and `tokens`.\n",
    "\n",
    "In addition, we store the index (or 'position') of each document and sentence under the variables `doc_ix` and `sent_ix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for doc_ix, annotation in enumerate(annotations):\n",
    "        \n",
    "    genre = annotation[0].metadata['meta::genre']\n",
    "    document_id = annotation[0].metadata['newdoc id']\n",
    "    \n",
    "    for sent_ix, sentence in enumerate(annotation):\n",
    "        \n",
    "        mood = sentence.metadata['s_type']\n",
    "        \n",
    "        data.append({'mood': mood, 'genre': genre, 'document_id': document_id, 'tokens': len(sentence), 'doc_ix': doc_ix, 'sent_ix': sent_ix})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac07c8c",
   "metadata": {},
   "source": [
    "This gives us a Python list populated by dictionaries.\n",
    "\n",
    "Let's use the `len` function to check its length â€“ this essentially gives us the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef078d9e",
   "metadata": {},
   "source": [
    "Just to illustrate how we can recover linguistic data from the list `annotations`, let's fetch the sentence stored at index 235 in the list `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[235]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387097d2",
   "metadata": {},
   "source": [
    "The values stored under the keys `doc_ix` and `sent_ix` can be used to access the information stored in the list `annotations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66121b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[data[235]['doc_ix']][data[235]['sent_ix']].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614111f",
   "metadata": {},
   "source": [
    "This illustrates how we are able to associate each sentence in the list `data` with the actual linguistic annotations.\n",
    "\n",
    "Next, we convert the list of dictionaries into a *pandas* DataFrame by calling the `DataFrame` class to which we provide the list of dictionaries `data` as input.\n",
    "\n",
    "We store the resulting DataFrame under the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae804563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd873c2d",
   "metadata": {},
   "source": [
    "DataFrame is a tabular format for storing various types of data.\n",
    "\n",
    "Let's call the variable `df` to examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066acb4",
   "metadata": {},
   "source": [
    "The DataFrame class has various useful functions for processing and manipulating tabular data.\n",
    "\n",
    "Let's use the `sample` method to draw a random sample of 10 sentences.\n",
    "\n",
    "For reproducibility, we also provide a value for the argument `random_state` - this number is used as the 'seed' for sampling and ensures we get the same result every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce8980",
   "metadata": {},
   "source": [
    "We can also count unique values in each column to get a better understanding of the data.\n",
    "\n",
    "Let's count how many sentences we have from each genre.\n",
    "\n",
    "To do so, we use the brackets and the string `genre` (note the single quotes that mark a string in Python) to select this column. We then call the `value_counts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23947cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d75f81",
   "metadata": {},
   "source": [
    "We can easily do the same for mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e80765",
   "metadata": {},
   "source": [
    "Whereas the columns *mood* and *genre* consist of categorical values, the column *tokens* contains numerical values. \n",
    "\n",
    "We can analyse them using the `describe` method, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e50eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d5269",
   "metadata": {},
   "source": [
    "The count gives the number of sentences included in the calculation. The remaining values provide the following information:\n",
    "\n",
    " - mean: the average number of tokens per sentence\n",
    " - std: standard deviation, which indicates the spread of the data around the mean\n",
    " - min: the smallest value for tokens\n",
    " - 25%: 25% of the data have 7 tokens or less\n",
    " - 50%: 50% of the data have 15 tokens or less\n",
    " - 75%: 75% of the data have 25 tokens or less\n",
    " - max: the largest value for tokens\n",
    " \n",
    "We can also plot the number of tokens across sentences using a histogram.\n",
    "\n",
    "To do so, we call the `hist` method. For the argument `bins`, which describe the 'bins' into which the observations are placed.\n",
    "\n",
    "Here we use Python's `range` function to provide the bins with values that range from 1 to 136."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'].hist(bins=range(1, 136))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11e7c9",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "We can think of the DataFrame stored under the variable `df` as our **sampling frame**, that is, a potential source of linguistic data.\n",
    "\n",
    "For this dataframe, the **sampling unit** is a sentence.\n",
    "\n",
    "We can also think of the information stored in the columns *mood* and *genre* as **strata** for sampling the sentences.\n",
    "\n",
    "As illustrated above, we can use the `sample` method in *pandas* to draw samples from the DataFrame.\n",
    "\n",
    "By providing the argument `n`, we can draw a random sample of *n* sentences from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=300, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4de58e",
   "metadata": {},
   "source": [
    "If we wish to draw a certain percentage of the data, we must use the argument `frac` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a8536-7c5c-4c4f-9cb0-a8472f340193",
   "metadata": {},
   "source": [
    "If we want to sample every fifth row in the DataFrame, we must use the `iloc` method.\n",
    "\n",
    "In the expression `[::5]`, the first colon indicates that we are taking 'slices' of the rows, whereas the second part `:5` means that we take every fifth row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eceb38-0ae5-40d6-974c-2ab06231ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa03cfe",
   "metadata": {},
   "source": [
    "What if we would like to draw a **balanced sample** for each genre, which means that each genre would be equally represented in the data?\n",
    "\n",
    "To do so, we can group the sentences according to their genre using the `groupby` function, which takes the column name to be used as the basis for grouping as input.\n",
    "\n",
    "We store the resulting groups under the variable `genres`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = df.groupby('genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea2943",
   "metadata": {},
   "source": [
    "Next, we create an empty DataFrame and store it under the variable `balanced_genres`.\n",
    "\n",
    "This will be used to store our balanced sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_genres = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab8bf2",
   "metadata": {},
   "source": [
    "Next, we loop over `genres`: each group consists of its name and the actual group of DataFrame rows.\n",
    "\n",
    "We refer to them as `name` and `genre` during the loop.\n",
    "\n",
    "In the loop, we draw a random sample of 200 sentences from each genre and store the result under the variable `sample`.\n",
    "\n",
    "We then use the `concat` function from pandas to concatenate our newly created DataFrame `balanced_genres` and the current `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, genre in genres:\n",
    "    \n",
    "    sample = genre.sample(n=200, random_state=42)\n",
    "    \n",
    "    balanced_genres = pd.concat([balanced_genres, sample], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e79f6",
   "metadata": {},
   "source": [
    "Let's examine the result by calling the variable `balanced_genres`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feef833",
   "metadata": {},
   "source": [
    "As you can see, the new DataFrame uses the indices (first column) from the old DataFrame.\n",
    "\n",
    "We can reset the index using the following command. The argument `drop` deletes the old index, whereas `inplace` modifies the existing DataFrame rather than returning a new one.\n",
    "\n",
    "Calling the variable shows that the index has been reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed730d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_genres.reset_index(drop=True, inplace=True)\n",
    "\n",
    "balanced_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38de977-82e3-4c16-809f-1520e1da7e6f",
   "metadata": {},
   "source": [
    "Let's check that each genre is represented the same number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374a374-3f31-48f8-a7be-cd95663847ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_genres['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f3402",
   "metadata": {},
   "source": [
    "Let's assume that our balanced sample of **genres** provides us with a more accurate distribution of **mood** among the sentences â€“ after all, we have much more data for some genres than others.\n",
    "\n",
    "Let's compare the counts for **mood** in the original data and the balanced sample of genres.\n",
    "\n",
    "Here we provide the argument `normalize` and set it to `True` to return percentages rather than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mood'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_genres['mood'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d425bc9",
   "metadata": {},
   "source": [
    "As you can see, the percentages are slightly different.\n",
    "\n",
    "What if we would like to sample the original data for **mood** based on their proportions in the balanced sample?\n",
    "\n",
    "Turns out we can use the percentages for the balanced sample as *weights* for sampling!\n",
    "\n",
    "You can think of these weights as reflecting the probability of each sentence being included in the sample.\n",
    "\n",
    "Let's store the percentages under the variable `mood_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28966210",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_weights = balanced_genres['mood'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588c381",
   "metadata": {},
   "source": [
    "Next, we use the `sample` method of a DataFrame to draw a sample of 1000 sentences from the sampling frame.\n",
    "\n",
    "The `sample` method has the argument `weights`, which expects a weight to be associated with each row in the DataFrame.\n",
    "\n",
    "We achieve this by mapping the value for **mood** in each column to the weights stored under `mood_weights`.\n",
    "\n",
    "Put differently, we look up the probability for each mood and use this value as the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66948c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_sample = df.sample(n=1000, weights=df['mood'].map(mood_weights), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640412e",
   "metadata": {},
   "source": [
    "Let's examine the resulting sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071742",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_sample['mood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc7099-ba4b-49b3-bded-a087fb688d38",
   "metadata": {},
   "source": [
    "Without the weights, each row has an equal probability of being sampled.\n",
    "\n",
    "When using the weights, there is a ~66% chance that a random row drawn from the DataFrame represents the category `decl`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada346b-1885-402d-a26c-07cb9f4eeb90",
   "metadata": {},
   "source": [
    "## Estimating sample sizes\n",
    "\n",
    "Let's continue by estimating sample sizes, that is, whether a corpus is large enough for studying some linguistic feature.\n",
    "\n",
    "As pointed out by Egbert et al. ([2023](https://doi.org/10.1017/9781316584880), p. 130), estimating sample size is useful for answering two questions:\n",
    "\n",
    "1. How many units of analysis (e.g. 'texts' or sentences) would be needed to reliably estimate the distribution of a linguistic feature?\n",
    "2. Can an existing corpus be used to estimate the distribution of a linguistic feature?\n",
    "\n",
    "Let's start by answering question #1 by using the sample stored under `mood_sample` to estimate the frequency of linguistic variables.\n",
    "\n",
    "As a first step, let's start by retrieving the linguistic annotations using the indices stored in the DataFrame `mood_sample`.\n",
    "\n",
    "First we convert the columns `doc_ix` and `sent_ix` of the DataFrame into Python lists.\n",
    "\n",
    "We then use the `zip` function to combine these two lists, and iterate over these pairs, which we then use to retrieve items from the list `annotations`.\n",
    "\n",
    "We store the resulting sentences into a list named `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe254b-2224-4eb4-9e3a-d207ac8d63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ixs, sent_ixs = mood_sample['doc_ix'].tolist(), mood_sample['sent_ix'].tolist()\n",
    "\n",
    "sentences = [annotations[doc_ix][sent_ix] for doc_ix, sent_ix in zip(doc_ixs, sent_ixs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ab74d-3aa9-4a1c-b269-a526ee624891",
   "metadata": {},
   "source": [
    "Let's print out the first five sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d259ed-3b92-4cbe-9cdc-b39bcb5e4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50bcff-1012-4ada-953f-df5d67be4e75",
   "metadata": {},
   "source": [
    "To estimate the size of a sample, we need to measure how frequently a linguistic feature occurs.\n",
    "\n",
    "To do so, we define a Python function that takes the following inputs:\n",
    "\n",
    "- a list of sentences annotated using the CoNLL-U schema and parsed using *conllu* (a list of `TokenList` objects)\n",
    "- the name of a linguistic feature as a Python string (e.g. `'deprel'`, `'upos'`)\n",
    "- the name of a tag as a Python string (e.g. `'advmod'`, `'NOUN'`)\n",
    "\n",
    "This function returns the **mean** number of features per 1000 words and its **standard deviation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8c397-83ea-4d78-bdd6-6340caf0b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_per_1000_words(sentences, feat, tag):\n",
    "    \n",
    "    # Initialize an empty list to store occurrences per 1000 words\n",
    "    occurrences_per_k = []\n",
    "    \n",
    "    # Initialize a counter for the current number of occurrences within the current 1000 words\n",
    "    current_k = 0\n",
    "    \n",
    "    # Initialize a counter for the total number of words processed\n",
    "    word_counter = 0\n",
    "    \n",
    "    # Iterate through each sentence in the input list of sentences\n",
    "    for sent in sentences:\n",
    "        \n",
    "        # Iterate through each token in the current sentence\n",
    "        for token in sent:\n",
    "            \n",
    "            # Check if the token's tag matches the specified feature\n",
    "            if token[feat] == tag:\n",
    "                \n",
    "                # If it does, increment the current count of occurrences\n",
    "                current_k += 1\n",
    "            \n",
    "            # Increment the total word counter regardless of the tag match\n",
    "            word_counter += 1\n",
    "            \n",
    "            # Check if the total word count has reached or exceeded 1000\n",
    "            if word_counter >= 1000:\n",
    "                \n",
    "                # If so, add the current count of occurrences to the list\n",
    "                occurrences_per_k.append(current_k)\n",
    "                \n",
    "                # Reset both the current count of occurrences and the total word counter\n",
    "                current_k = 0\n",
    "                word_counter = 0\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the occurrences per 1000 words using NumPy\n",
    "    return np.mean(occurrences_per_k), np.std(occurrences_per_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3b598-c29f-4482-8ca8-b3d58fbfe6bc",
   "metadata": {},
   "source": [
    "Egbert et al. ([2023](https://doi.org/10.1017/9781316584880), p. 130) provide the following formula for estimating the sample size needed for studying a particular linguistic feature:\n",
    "\n",
    "$\\large n = \\frac{\\large stdev^2}{(\\frac{.5 \\times \\text{CI range}}{t})^2}$\n",
    "\n",
    "Where *n* is the required sample size, *stdev* is the standard deviation, *CI range* is the range of the confidence interval and *t* is the *t*-value for the desired probability level.\n",
    "\n",
    "For current purposes, we will use a *t*-value of 1.96, which corresponds to a 95% probability that the true mean value of the observed linguistic feature falls within the confidence interval.\n",
    "\n",
    "Let's start by calculating the mean and standard deviation for nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db33b0-1208-477f-a625-89d9efa1ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, stdev = calculate_mean_per_1000_words(sentences, 'upos', 'NOUN')\n",
    "\n",
    "mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329556dc-1433-4597-a5d1-2f319f36ee79",
   "metadata": {},
   "source": [
    "Biber ([1993](https://doi.org/10.1093/llc/8.4.243), p. 253) recommends using $\\pm 5$% of the observed mean value as the range for the confidence interval.\n",
    "\n",
    "The upper and lower boundaries may be calculated by multiplying the mean value by 1.05 and 0.95, respectively.\n",
    "\n",
    "To get the actual range for the confidence interval, we must deduct the lower boundary from the upper boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712fca4-580d-4f78-a1cf-e9a0008fd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_range = mean * 1.05 - mean * 0.95\n",
    "\n",
    "ci_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ba76e-0a81-4605-b3d8-ccd4af5256af",
   "metadata": {},
   "source": [
    "Now we have all the necessary values for filling the formula for estimating the required sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99357d64-5f51-46a7-b7af-7baf85bbe75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (stdev ** 2) / ((0.5 * ci_range / 1.96) ** 2)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a698d-deb1-4ef3-8ca3-50e0d07c9ea2",
   "metadata": {},
   "source": [
    "Just 12 sentences would suffice for reliably estimating the distribution of nouns!\n",
    "\n",
    "What if you replace the tag `NOUN` with a rare tag, such as `SYM`?\n",
    "\n",
    "What about dependencies `deprel` such as `nsubj` (nominal subject) or `nsubj:pass` (nominal subject in a passive construction)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddf1dd-87ab-46ff-8d93-b4102634e451",
   "metadata": {},
   "source": [
    "To estimate whether an existing corpus can be used to estimate the distribution of a linguistic feature, we can use a measure known as **standard error** (Egbert et al. [2023](https://doi.org/10.1017/9781316584880), p. 134).\n",
    "\n",
    "Standard error measures how far the mean value for a sample is likely to be from the true mean value.\n",
    "\n",
    "This measure is calculated by dividing the standard deviation with the square root of the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b395f-b35f-4950-a69a-04751cd03e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error = (stdev / np.sqrt(1000))\n",
    "\n",
    "standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45add6a8-bc02-4457-939d-e71d39d0e246",
   "metadata": {},
   "source": [
    "To compare standard errors between linguistic features, we can calculate a measure known as **relative standard error**.\n",
    "\n",
    "This measure is calculated by dividing the standard error with the mean value for a linguistic feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b286e4-ab1e-4657-96ba-d4f93edd556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_standard_error = standard_error / mean\n",
    "\n",
    "relative_standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf44f9-d287-467e-9b78-7335f9415aed",
   "metadata": {},
   "source": [
    "Egbert et al. (2023, p. 135) provide the following critical values for error rates and relative standard error (RSE):\n",
    "\n",
    "| Error rate (as a percentage of the mean) | RSE   |\n",
    "|------------------------------------------|-------|\n",
    "| 10%                                      | 0.051 |\n",
    "| 5%                                       | 0.0255|\n",
    "| 1%                                       | 0.0051|\n",
    "\n",
    "A relative standard error of 0.0029 for nouns indicates that the true mean value for nouns is likely to fall within 1% of the sample mean.\n",
    "\n",
    "This allows for reliable or 'precise' estimates concerning the distribution of nouns.\n",
    "\n",
    "What happens to RSE if you adjust the sample size?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
